{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNP4t8ZZCNGL"
      },
      "source": [
        "# **Spam Email Classifier Using Machine Learning**\n",
        "\n",
        "## **Project Description**:\n",
        "\n",
        "This project aims to develop a spam email classifier using machine learning techniques to automatically categorize incoming emails into spam and non-spam (ham). The dataset consists of 33,715 email samples labeled as spam or not. The goal is to train a machine learning model that can accurately classify emails into these two categories.\n",
        "\n",
        "To evaluate the model's performance, the dataset is split into training and testing sets. In this project, 70% of the data (23,570 samples) is used for training the model, and 30% (10,145 samples) is reserved for testing the model's ability to generalize to unseen data.\n",
        "\n",
        "### The following steps are followed in the project:\n",
        "\n",
        "**Data Preprocessing**: Cleaning and preparing the dataset, including text normalization (removal of stop words, punctuation, etc.).\n",
        "\n",
        "**Feature Extraction**: Converting email text data into numerical features, such as word frequency (TF-IDF), to feed into machine learning algorithms.\n",
        "\n",
        "**Model Selection**: Testing various classification algorithms (**e.g.**, **Logistic Regression**, **Naive Bayes**, **SVM**) to determine the best-performing model.\n",
        "\n",
        "**Model Evaluation**: Using standard metrics (**accuracy**, **precision**, **recall**, **F1-score**) to assess model performance on the test set.\n",
        "\n",
        "**Optimization**: Fine-tuning the model to improve accuracy and reduce overfitting.\n",
        "By the end of the project, a reliable spam filter will be developed that can automatically classify emails, contributing to more efficient email management.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xh9E1RSXRVq4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "import re\n",
        "import joblib\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z_TQjlSJYAS8"
      },
      "outputs": [],
      "source": [
        "email_df = pd.read_csv('enron-spam.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgSskmp6-ZKP",
        "outputId": "5a235a1d-e1b6-40ec-d31e-e46195d2c84a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyKesaz58QIM"
      },
      "outputs": [],
      "source": [
        "class EmailPreProcessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor for EmailPreProcessor, takes in no arguments.\n",
        "\n",
        "        Initializes the necessary components for email preprocessing:\n",
        "        - stopwords: A set of common English stopwords.\n",
        "        - stemmer: The Porter Stemmer for stemming words.\n",
        "        - vectorizer: A TF-IDF vectorizer to convert text to numerical features.\n",
        "        \"\"\"\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        The fit method is responsible for learning the vocabulary and the TF-IDF scores\n",
        "        from the email dataset. It also performs basic text preprocessing steps like:\n",
        "        - Cleaning email content (removes HTML tags, special characters, and punctuation).\n",
        "        - Removing stopwords.\n",
        "        - Applying stemming.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pandas DataFrame containing email content in the 'content' column.\n",
        "        - y: Optional; target labels (not used here).\n",
        "\n",
        "        Returns:\n",
        "        - self: The fitted EmailPreProcessor object.\n",
        "        \"\"\"\n",
        "\n",
        "        email_df = X.copy(deep=True)\n",
        "\n",
        "        email_df['content'] = self.clean_email_content(email_df)\n",
        "        email_df['tags'] = self.stopwords_removal(email_df)\n",
        "        email_df['tags'] = self.stem_conversion(email_df)\n",
        "\n",
        "        self.vectorizer.fit(email_df['tags'].values)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame, y=None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        The transform method preprocesses the email data by applying the learned\n",
        "        transformations (e.g., TF-IDF) on the raw email content.\n",
        "\n",
        "        Preprocessing steps:\n",
        "        - Removing HTML tags, special characters, punctuation.\n",
        "        - Lowercasing the text.\n",
        "        - Removing stopwords.\n",
        "        - Stemming the tokens.\n",
        "        - Applying TF-IDF vectorization.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pandas DataFrame containing the email content in the 'content' column.\n",
        "        - y: Optional; target labels (not used here).\n",
        "\n",
        "        Returns:\n",
        "        - A matrix with TF-IDF values representing the transformed email data.\n",
        "        \"\"\"\n",
        "\n",
        "        email_df = X.copy(deep=True)\n",
        "\n",
        "        email_df['content'] = self.clean_email_content(email_df)\n",
        "        email_df['tags'] = self.stopwords_removal(email_df)\n",
        "        email_df['tags'] = self.stem_conversion(email_df)\n",
        "\n",
        "        return self.vectorizer.transform(email_df['tags'].values).toarray()\n",
        "\n",
        "\n",
        "    def clean_email_content(self, email_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Removes html tags, unwanted special characters and punctuations and extra spaces\n",
        "        from email content.\n",
        "\n",
        "        paramters: email_df -> pandas Dataframe\n",
        "        \"\"\"\n",
        "        def clean_text(text: str) -> str:\n",
        "            # remove HTML tags using BeautifulSoup\n",
        "            text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "            # remove non-ASCII characters and unwanted symbols (e.g., \"\\x01\")\n",
        "            text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "            # remove all punctuation using regex (except spaces)\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "            # normalize multiple spaces or newlines to a single space\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            # convert the text string to lower case for noramlization\n",
        "            text = text.lower()\n",
        "\n",
        "            return text\n",
        "\n",
        "        return email_df['content'].apply(clean_text)\n",
        "\n",
        "\n",
        "    def stopwords_removal(self, email_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Removes stopwords from the email text.\n",
        "\n",
        "        Parameters:\n",
        "        - email_df: pandas DataFrame containing the 'content' column with tokenized text.\n",
        "\n",
        "        Returns:\n",
        "        - A pandas Series with the tokenized text after removing stopwords.\n",
        "        \"\"\"\n",
        "        def stopwords_removal_helper(text):\n",
        "            words = text.split()\n",
        "            return [word for word in words if word not in self.stop_words]\n",
        "\n",
        "        return email_df['content'].apply(stopwords_removal_helper)\n",
        "\n",
        "\n",
        "    def stem_conversion(self, email_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Applies stemming to the tokens in the email content.\n",
        "\n",
        "        Parameters:\n",
        "        - email_df: pandas DataFrame containing tokenized email content (tags).\n",
        "\n",
        "        Returns:\n",
        "        - A pandas Series with the stemmed words.\n",
        "        \"\"\"\n",
        "        def stem_conversion_helper(words):\n",
        "            # join the list values as a string to pass it to TF-IDF Vectorizer\n",
        "            return \" \".join([self.stemmer.stem(word) for word in words])\n",
        "\n",
        "        return email_df['tags'].apply(stem_conversion_helper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_Bv5op9ARN"
      },
      "source": [
        "## Model Training.\n",
        "\n",
        "### Train Test split\n",
        "We use 70% of the data for training and remaining 30% for testing, to prevent overfitting and generalize the model's ability to perform better for unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unOu7jOi-mmu"
      },
      "outputs": [],
      "source": [
        "# defining the input and target columns\n",
        "input_cols = ['content']\n",
        "target_col = ['spam']\n",
        "\n",
        "X = email_df[input_cols]\n",
        "y = email_df[target_col]\n",
        "\n",
        "# split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGxOCagLER2s"
      },
      "source": [
        "### Multinomial Naive bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbsUQ8ac_hNr",
        "outputId": "03dcbb46-80b2-465c-8e6f-83b57b3a981b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ],
      "source": [
        "# Pipeline for naive bayes model.\n",
        "naive_bayes_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', EmailPreProcessor()),\n",
        "        ('multinomial_nb_clf', MultinomialNB(alpha=1, fit_prior=False))\n",
        "    ]\n",
        ")\n",
        "\n",
        "naive_bayes_pipeline.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "y_train_pred = naive_bayes_pipeline.predict(X_train)\n",
        "y_test_pred = naive_bayes_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F9tQ0MTMYFO"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj6DeLvkL-dY",
        "outputId": "9875e625-62f1-46b7-f825-a0a73c8fee71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     11572\n",
            "           1       0.99      0.99      0.99     12028\n",
            "\n",
            "    accuracy                           0.99     23600\n",
            "   macro avg       0.99      0.99      0.99     23600\n",
            "weighted avg       0.99      0.99      0.99     23600\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      4973\n",
            "           1       0.99      0.98      0.98      5142\n",
            "\n",
            "    accuracy                           0.98     10115\n",
            "   macro avg       0.98      0.98      0.98     10115\n",
            "weighted avg       0.98      0.98      0.98     10115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_train.values.ravel(), y_train_pred))\n",
        "print(classification_report(y_test.values.ravel(), y_test_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BribgtcGRLfZ"
      },
      "source": [
        "## Exporting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNheAZqnRRnv",
        "outputId": "cf7a7d85-1214-4c3f-d353-d1dd71d3f4b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['spam_email_clf_nb.joblib']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(naive_bayes_pipeline, 'spam_email_clf_nb.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dl-4ZA3uW9Z"
      },
      "source": [
        "## For Testing purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDv_c-X-uayK",
        "outputId": "107030b4-29a6-4d99-88cf-39e1674ca017"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-7-fd56902782be>:81: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ],
      "source": [
        "preprocessor = EmailPreProcessor()\n",
        "preprocessor.fit(email_df)\n",
        "X = preprocessor.transform(email_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdyH9kp6vv4J",
        "outputId": "362e059f-e740-429f-b6e9-670e9634988a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(33715, 132422)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf = SVC(C=1.5, kernel='rbf')\n",
        "svc_clf.fit(X, y.values.ravel())\n",
        "\n",
        "\n",
        "svc_clf.score(X, y.values.ravel())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
